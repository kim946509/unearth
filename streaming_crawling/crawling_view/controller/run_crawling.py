"""
ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (ìš´ì˜ìš©)
"""
import sys
import os
import django
import logging
import time
from datetime import datetime, date
from collections import defaultdict

# Django ì„¤ì •
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings')
django.setup()

from crawling_view.controller.crawling_manager import run_crawling, run_platform_crawling
from crawling_view.data.song_service import SongService
from crawling_view.utils.constants import Platforms

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/crawling_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def run_full_crawling(target_date=None):
    """
    ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ (ìš´ì˜ìš©)
    
    Args:
        target_date (date, optional): í¬ë¡¤ë§ ëŒ€ìƒ ë‚ ì§œ. Noneì´ë©´ ì˜¤ëŠ˜ ë‚ ì§œ
        
    Returns:
        dict: ìƒì„¸í•œ í¬ë¡¤ë§ ê²°ê³¼
    """
    start_time = time.time()
    start_datetime = datetime.now()
    
    logger.info("ğŸš€ ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
    logger.info(f"ğŸ“… í¬ë¡¤ë§ ëŒ€ìƒ ë‚ ì§œ: {target_date or date.today()}")
    logger.info(f"â° ì‹œì‘ ì‹œê°„: {start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # í¬ë¡¤ë§ ì‹¤í–‰
        result = run_crawling(target_date)
        
        # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
        end_time = time.time()
        end_datetime = datetime.now()
        elapsed_time = end_time - start_time
        
        # ê²°ê³¼ ë¶„ì„
        analysis = analyze_crawling_result(result, elapsed_time, start_datetime, end_datetime)
        
        # ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        log_detailed_results(analysis)
        
        return analysis
        
    except Exception as e:
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        logger.error(f"âŒ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        
        error_result = {
            'status': 'error',
            'error_message': str(e),
            'start_time': start_datetime,
            'end_time': datetime.now(),
            'elapsed_time': elapsed_time,
            'target_date': target_date or date.today()
        }
        
        return error_result

def analyze_crawling_result(result, elapsed_time, start_datetime, end_datetime):
    """
    í¬ë¡¤ë§ ê²°ê³¼ ìƒì„¸ ë¶„ì„
    
    Args:
        result (dict): í¬ë¡¤ë§ ê²°ê³¼
        elapsed_time (float): ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
        start_datetime (datetime): ì‹œì‘ ì‹œê°„
        end_datetime (datetime): ì¢…ë£Œ ì‹œê°„
        
    Returns:
        dict: ë¶„ì„ëœ ê²°ê³¼
    """
    analysis = {
        'status': result.get('status', 'unknown'),
        'start_time': start_datetime,
        'end_time': end_datetime,
        'elapsed_time': elapsed_time,
        'target_date': result.get('target_date'),
        'total_songs': result.get('total_songs', 0),
        'platforms': {},
        'summary': {}
    }
    
    if result['status'] == 'success':
        # í”Œë«í¼ë³„ ìƒì„¸ ë¶„ì„
        crawling_results = result.get('crawling_results', {})
        db_results = result.get('db_results', {})
        csv_results = result.get('csv_results', {})
        
        total_crawled = 0
        total_saved_db = 0
        total_saved_csv = 0
        total_failed = 0
        
        for platform in Platforms.ALL_PLATFORMS:
            platform_data = {
                'crawled_count': 0,
                'db_saved': 0,
                'db_updated': 0,
                'db_failed': 0,
                'db_skipped': 0,
                'csv_saved': 0,
                'status': 'not_executed'
            }
            
            # í¬ë¡¤ë§ ê²°ê³¼ ë¶„ì„
            if platform in crawling_results:
                platform_data['status'] = 'success'
                crawled_data = crawling_results[platform]
                
                if isinstance(crawled_data, list):
                    platform_data['crawled_count'] = len(crawled_data)
                elif isinstance(crawled_data, dict):
                    platform_data['crawled_count'] = len(crawled_data)
                
                total_crawled += platform_data['crawled_count']
            
            # DB ì €ì¥ ê²°ê³¼ ë¶„ì„
            if platform in db_results:
                db_result = db_results[platform]
                if isinstance(db_result, dict):
                    platform_data['db_saved'] = db_result.get('saved_count', 0)
                    platform_data['db_updated'] = db_result.get('updated_count', 0)
                    platform_data['db_failed'] = db_result.get('failed_count', 0)
                    platform_data['db_skipped'] = db_result.get('skipped_count', 0)
                    
                    total_saved_db += platform_data['db_saved'] + platform_data['db_updated']
                    total_failed += platform_data['db_failed']
            
            # CSV ì €ì¥ ê²°ê³¼ ë¶„ì„
            if platform in csv_results:
                csv_result = csv_results[platform]
                if isinstance(csv_result, list):
                    platform_data['csv_saved'] = len(csv_result)
                    total_saved_csv += platform_data['csv_saved']
            
            analysis['platforms'][platform] = platform_data
        
        # ì „ì²´ ìš”ì•½
        # ì´ í¬ë¡¤ë§ ì‹œë„: ê³¡ ìˆ˜ Ã— í”Œë«í¼ ìˆ˜
        total_attempts = analysis['total_songs'] * len(Platforms.ALL_PLATFORMS)
        
        # ì‹¤ì œ ì„±ê³µ: CSV ì €ì¥ì´ ì„±ê³µí•œ ê²ƒì´ ì‹¤ì œ ì„±ê³µ
        actual_success = total_saved_csv
        
        # ì‹¤íŒ¨: ì´ ì‹œë„ - ì‹¤ì œ ì„±ê³µ
        total_failed = total_attempts - actual_success
        
        # ì„±ê³µë¥ : ì‹¤ì œ ì„±ê³µ / ì´ ì‹œë„
        success_rate = (actual_success / max(total_attempts, 1)) * 100
        
        analysis['summary'] = {
            'total_crawled': total_crawled,
            'total_saved_db': total_saved_db,
            'total_saved_csv': total_saved_csv,
            'total_failed': total_failed,
            'success_rate': success_rate
        }
    
    return analysis

def log_detailed_results(analysis):
    """
    ìƒì„¸í•œ ê²°ê³¼ë¥¼ ë¡œê·¸ë¡œ ì¶œë ¥
    
    Args:
        analysis (dict): ë¶„ì„ëœ ê²°ê³¼
    """
    logger.info("=" * 80)
    logger.info("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìƒì„¸ ë¶„ì„")
    logger.info("=" * 80)
    
    # ê¸°ë³¸ ì •ë³´
    logger.info(f"ğŸ“… ëŒ€ìƒ ë‚ ì§œ: {analysis['target_date']}")
    logger.info(f"â° ì‹œì‘ ì‹œê°„: {analysis['start_time'].strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"â° ì¢…ë£Œ ì‹œê°„: {analysis['end_time'].strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"â±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: {analysis['elapsed_time']:.2f}ì´ˆ ({analysis['elapsed_time']/60:.2f}ë¶„)")
    
    if analysis['status'] == 'success':
        logger.info(f"ğŸµ ì „ì²´ ëŒ€ìƒ ê³¡: {analysis['total_songs']}ê°œ")
        
        # í”Œë«í¼ë³„ ê²°ê³¼
        logger.info("\nğŸ“ˆ í”Œë«í¼ë³„ ìƒì„¸ ê²°ê³¼:")
        logger.info("-" * 60)
        
        for platform, data in analysis['platforms'].items():
            status_emoji = "âœ…" if data['status'] == 'success' else "âŒ" if data['status'] == 'error' else "âš ï¸"
            logger.info(f"{status_emoji} {platform.upper()}:")
            logger.info(f"   í¬ë¡¤ë§: {data['crawled_count']}ê°œ")
            logger.info(f"   DB ì €ì¥: {data['db_saved']}ê°œ ìƒì„±, {data['db_updated']}ê°œ êµì²´, {data['db_failed']}ê°œ ì‹¤íŒ¨, {data['db_skipped']}ê°œ ìŠ¤í‚µ")
            logger.info(f"   CSV ì €ì¥: {data['csv_saved']}ê°œ íŒŒì¼")
        
        # ì „ì²´ ìš”ì•½
        summary = analysis['summary']
        total_attempts = analysis['total_songs'] * len(Platforms.ALL_PLATFORMS)
        
        logger.info("\nğŸ“Š ì „ì²´ ìš”ì•½:")
        logger.info("-" * 60)
        logger.info(f"ğŸµ ëŒ€ìƒ ê³¡: {analysis['total_songs']}ê°œ")
        logger.info(f"ğŸŒ í”Œë«í¼: {len(Platforms.ALL_PLATFORMS)}ê°œ")
        logger.info(f"ğŸ¯ ì´ í¬ë¡¤ë§ ì‹œë„: {total_attempts}ê°œ")
        logger.info(f"ğŸ’¾ DB ì €ì¥: {summary['total_saved_db']}ê°œ")
        logger.info(f"ğŸ“„ CSV ì €ì¥: {summary['total_saved_csv']}ê°œ íŒŒì¼")
        logger.info(f"âŒ ì‹¤íŒ¨: {summary['total_failed']}ê°œ")
        logger.info(f"ğŸ“ˆ ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
        
        # ì„±ëŠ¥ ë¶„ì„
        if summary['total_crawled'] > 0:
            avg_time_per_song = analysis['elapsed_time'] / summary['total_crawled']
            logger.info(f"âš¡ ê³¡ë‹¹ í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time_per_song:.2f}ì´ˆ")
    
    elif analysis['status'] == 'no_songs':
        logger.warning("âš ï¸ í¬ë¡¤ë§ ëŒ€ìƒ ê³¡ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    elif analysis['status'] == 'error':
        logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {analysis.get('error_message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
    
    logger.info("=" * 80)

def run_single_platform_crawling(platform, target_date=None):
    """
    ë‹¨ì¼ í”Œë«í¼ í¬ë¡¤ë§ ì‹¤í–‰
    
    Args:
        platform (str): í”Œë«í¼ëª… ('genie', 'youtube', 'youtube_music', 'melon')
        target_date (date, optional): í¬ë¡¤ë§ ëŒ€ìƒ ë‚ ì§œ
        
    Returns:
        dict: í¬ë¡¤ë§ ê²°ê³¼
    """
    start_time = time.time()
    start_datetime = datetime.now()
    
    logger.info(f"ğŸš€ {platform.upper()} í”Œë«í¼ í¬ë¡¤ë§ ì‹œì‘")
    logger.info(f"ğŸ“… í¬ë¡¤ë§ ëŒ€ìƒ ë‚ ì§œ: {target_date or date.today()}")
    logger.info(f"â° ì‹œì‘ ì‹œê°„: {start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        result = run_platform_crawling(platform, target_date)
        
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        logger.info(f"âœ… {platform.upper()} í¬ë¡¤ë§ ì™„ë£Œ")
        logger.info(f"â±ï¸  ì‹¤í–‰ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
        
        if result['status'] == 'success':
            crawling_count = len(result.get('crawling_results', []))
            logger.info(f"ğŸ¯ í¬ë¡¤ë§ ì„±ê³µ: {crawling_count}ê°œ")
            
            db_result = result.get('db_results', {})
            if isinstance(db_result, dict):
                saved_count = db_result.get('saved_count', 0)
                updated_count = db_result.get('updated_count', 0)
                failed_count = db_result.get('failed_count', 0)
                logger.info(f"ğŸ’¾ DB ì €ì¥: {saved_count}ê°œ ìƒì„±, {updated_count}ê°œ êµì²´, {failed_count}ê°œ ì‹¤íŒ¨")
            
            csv_result = result.get('csv_results', [])
            if isinstance(csv_result, list):
                logger.info(f"ğŸ“„ CSV ì €ì¥: {len(csv_result)}ê°œ íŒŒì¼")
        
        return result
        
    except Exception as e:
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        logger.error(f"âŒ {platform.upper()} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}", exc_info=True)
        logger.info(f"â±ï¸  ì‹¤í–‰ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
        
        return {'status': 'error', 'platform': platform, 'error_message': str(e)}

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰')
    parser.add_argument('--platform', choices=Platforms.ALL_PLATFORMS, 
                       help='íŠ¹ì • í”Œë«í¼ë§Œ í¬ë¡¤ë§')
    parser.add_argument('--date', type=str, help='í¬ë¡¤ë§ ëŒ€ìƒ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)')
    
    args = parser.parse_args()
    
    # ë‚ ì§œ íŒŒì‹±
    target_date = None
    if args.date:
        try:
            target_date = datetime.strptime(args.date, '%Y-%m-%d').date()
        except ValueError:
            logger.error("âŒ ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. YYYY-MM-DD í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            sys.exit(1)
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    if args.platform:
        # ë‹¨ì¼ í”Œë«í¼ í¬ë¡¤ë§
        result = run_single_platform_crawling(args.platform, target_date)
    else:
        # ì „ì²´ í¬ë¡¤ë§
        result = run_full_crawling(target_date)
    
    # ì¢…ë£Œ ì½”ë“œ ì„¤ì •
    if result.get('status') == 'success':
        sys.exit(0)
    else:
        sys.exit(1) 